import os
import re
import asyncio
import requests
from bs4 import BeautifulSoup
from ddgs import DDGS
from openai import AsyncOpenAI
from typing import List, Dict, Any


# è¨­å®šçˆ¬å–å…§å®¹é•·åº¦é™åˆ¶ (é¿å…è¶…é Context Window)
MAX_CHARS_PER_PAGE = 3000


def _simplify_query(raw: str, fallback: str) -> str:
    """å°‡ LLM ç”¢ç”Ÿçš„é—œéµå­—å­—ä¸²ç°¡åŒ–æˆè¼ƒçŸ­ã€è¼ƒä¹¾æ·¨çš„æœå°‹ queryã€‚

    - ç§»é™¤å¤šé¤˜ç©ºç™½èˆ‡å¸¸è¦‹è´…è©ï¼ˆå¦‚ã€Œæ˜¯ä»€éº¼ã€ã€ã€Œå¦‚ä½•ã€ã€ã€Œè«‹å•ã€ç­‰ï¼‰ã€‚
    - åªä¿ç•™å‰å¹¾å€‹é—œéµè©ï¼Œé¿å… query éé•·ã€éé›œã€‚
    """

    s = re.sub(r"\s+", " ", raw).strip()
    if not s:
        return fallback

    # ä¾æ¨™é»èˆ‡ç©ºç™½åˆ‡è©
    tokens = re.split(r"[,\u3001;ï¼Œã€‚ï¼ï¼Ÿ\?ã€\s]+", s)
    stopwords = {
        "æ˜¯ä»€éº¼", "æ˜¯ç”šéº¼", "ç‚ºä»€éº¼", "ç‚ºä½•", "å¦‚ä½•", "æ€éº¼", "æ€æ¨£",
        "è«‹å•", "å¹«æˆ‘", "ä»‹ç´¹", "èªªæ˜", "åˆ†æ", "è§£é‡‹", "çš„", "ä¸€ä¸‹",
    }

    filtered: List[str] = []
    for t in tokens:
        t = t.strip()
        if not t or t in stopwords:
            continue
        filtered.append(t)
        if len(filtered) >= 5:
            break

    if not filtered:
        return fallback

    return " ".join(filtered)

# =========================================

async def get_llm_decision_and_query(client: AsyncOpenAI, model_name: str, messages: List[Dict[str, str]]):
    """ï¼ˆç›®å‰æœªåœ¨ä¸»æµç¨‹ä½¿ç”¨ï¼‰

    ç¬¬ä¸€éšæ®µï¼šLLM åˆ¤æ–·æ˜¯å¦éœ€è¦æœç´¢ã€‚
    å¦‚æœéœ€è¦ï¼Œå›å‚³æœç´¢å­—ä¸²ï¼›å¦‚æœä¸éœ€è¦ï¼Œå›å‚³ç›´æ¥ç­”æ¡ˆã€‚
    ç‚ºäº†ç°¡åŒ–è§£æï¼Œæˆ‘å€‘è¦æ±‚ LLM ä½¿ç”¨ç‰¹å®šå‰ç¶´ã€‚
    """
    system_prompt = """
    You are a smart decision-making assistant.
    Determine if the user's request requires real-time information or external data (web search).

    Rules:
    1. If web search is needed (e.g., current events, weather, specific stats), output ONLY the best search keywords.Answer in Traditional Chinese.
    2. If no search is needed (e.g., general knowledge, coding, translation, chat), output ONLY the number "0".

    Do not provide any explanations or extra text.
    """
    # ä¸ç›´æ¥ä¿®æ”¹åŸ messagesï¼Œå»ºç«‹æ–°çš„ decision_messages
    decision_messages: List[Dict[str, str]] = [
        {"role": "system", "content": system_prompt}
    ] + list(messages)

    response = await client.chat.completions.create(
        model=model_name,
        messages=decision_messages,
        max_tokens=100,
        temperature=0.0,
    )
    
    content = response.choices[0].message.content.strip()

    # åˆ¤æ–·é‚è¼¯
    if content == "0":
        return False, None
    else:
        # å¦‚æœä¸æ˜¯ 0ï¼Œä»£è¡¨å…§å®¹å°±æ˜¯æœå°‹é—œéµå­—
        return True, content


async def extract_search_query(client: AsyncOpenAI, model_name: str, question: str) -> str:
    """è®“ LLM å¹«å¿™æŠŠä½¿ç”¨è€…å•é¡Œè½‰æˆé©åˆæœå°‹çš„é—œéµå­—ã€‚

    è¦å‰‡ï¼š
    - ä¸è¦ç›´æ¥å›ç­”å•é¡Œï¼Œåªè¼¸å‡ºé—œéµå­—ï¼ˆ5-20 å€‹å­—ä¹‹å…§ï¼‰ã€‚
    - å¯ä»¥ç”¨ç¹é«”ä¸­æ–‡æˆ–ä¸­è‹±æ··åˆï¼Œä½†ä»¥ç¹é«”ä¸­æ–‡ç‚ºä¸»ã€‚
    - ä¸è¦åŠ å‰å¾Œè§£é‡‹æ–‡å­—ï¼Œåªè¼¸å‡ºé—œéµå­—æœ¬èº«ã€‚
    """

    system_prompt = """
    You are a search query generator for a chatbot about Taiwan Indigenous Peoples (especially the Paiwan people).
    Given a user's question (likely in Traditional Chinese),
    generate a concise set of search keywords suitable for DuckDuckGo web search.

    Requirements:
    - Use Traditional Chinese when appropriate.
    - Focus on the core topic and related entities (people, places, organizations, languages, rituals).
    - If the question may relate to Taiwan Indigenous culture or rituals (e.g. åŒ…å«ã€Œäº”å¹´ç¥­ã€ã€ã€Œç¥­å…¸ã€ã€ã€Œç¥­å„€ã€ã€ã€Œéƒ¨è½ã€ã€ã€ŒåŸä½æ°‘ã€ã€ã€Œæ’ç£ã€ç­‰è©),
      then include relevant terms such asã€Œæ’ç£æ—ã€ã€ã€Œå°ç£åŸä½æ°‘ã€ã€ã€Œç¥­å„€ã€ã€ã€Œå‚³çµ±æ–‡åŒ–ã€ in the keywords.
    - Length: roughly 5 to 20 characters/words.
    - Do NOT answer the question.
    - Output ONLY the search keywords, with no extra explanation.
    """

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": question},
    ]

    try:
        response = await client.chat.completions.create(
            model=model_name,
            messages=messages,
            max_tokens=64,
            temperature=0.2,
        )
        query_raw = (response.choices[0].message.content or "").strip()
        if not query_raw:
            return question

        # å°‡ LLM ç”¢ç”Ÿçš„é—œéµå­—é€²ä¸€æ­¥ç°¡åŒ–ï¼Œé¿å… query éé•·æˆ–å¤ªé›œ
        query = _simplify_query(query_raw, fallback=question)
        return query
    except Exception as e:
        # ç™¼ç”ŸéŒ¯èª¤æ™‚é€€å›ç›´æ¥ç”¨åŸå§‹å•é¡Œæœå°‹ï¼Œé¿å…æ•´é«”æµç¨‹å¤±æ•—
        print(f"âš ï¸ extract_search_query å¤±æ•—ï¼Œæ”¹ç”¨åŸå§‹å•é¡Œï¼š{e}")
        return question

async def get_web_summary(client: AsyncOpenAI, model_name: str, messages: List[Dict[str, str]], query: str, max_results: int = 3) -> Dict[str, Any]:
    """æ•´åˆå‡½å¼ï¼šåŸ·è¡Œ æœå°‹ -> çˆ¬å– -> æ¿ƒç¸® çš„å®Œæ•´æµç¨‹ã€‚

    å›å‚³ï¼š{"summary": str, "sources": List[{"title": str, "url": str}]}
    """
    print(f"ğŸ” [æœå°‹] æ­£åœ¨ DuckDuckGo æŸ¥è©¢: {query} ...")
    
    # --- 1. åŸ·è¡Œæœå°‹ (ä½¿ç”¨ to_thread é¿å…å¡ä½) ---
    def run_search():
        results = []
        with DDGS() as ddgs:
            # é€™è£¡çš„ ddgs.text æ˜¯åŒæ­¥çš„ï¼Œæ‰€ä»¥åŒ…åœ¨å‡½å¼è£¡è·‘
            # region è¨­ç‚ºå°ç£ç¹é«”ï¼Œè®“çµæœæ›´åå‘åœ¨åœ°èˆ‡è¯æ–‡å…§å®¹
            search_gen = ddgs.text(query, max_results=max_results, region="tw-tzh")
            if search_gen:
                for r in search_gen:
                    results.append(r)
        return results

    # åœ¨èƒŒæ™¯åŸ·è¡Œæœå°‹
    search_results = await asyncio.to_thread(run_search)

    if not search_results:
        return {"summary": "æœå°‹ç„¡çµæœã€‚", "sources": []}

    # --- 2. åŸ·è¡Œçˆ¬å– (ä¾åºçˆ¬å–å‰ N ç­†) ---
    aggregated_content = ""
    used_sources: List[Dict[str, str]] = []
    
    for idx, res in enumerate(search_results):
        url = res['href']
        title = res['title']
        print(f"ğŸ“„ [çˆ¬å–] æ­£åœ¨è®€å–ç¬¬ {idx+1} ç­†: {title}")

        # å®šç¾©å–®ä¸€çˆ¬å–å‹•ä½œ (åŒæ­¥ç¨‹å¼ç¢¼)
        def fetch_one():
            try:
                headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"}
                resp = requests.get(url, headers=headers, timeout=5)
                resp.raise_for_status()
                soup = BeautifulSoup(resp.text, 'html.parser')
                for tag in soup(["script", "style", "nav", "footer", "iframe"]):
                    tag.extract()
                return soup.get_text(separator=' ', strip=True)[:MAX_CHARS_PER_PAGE]
            except Exception as e:
                print(f"âš ï¸ ç„¡æ³•è®€å– {url}: {e}")
                return ""

        # åœ¨èƒŒæ™¯åŸ·è¡Œçˆ¬å–
        content = await asyncio.to_thread(fetch_one)
        
        if content:
            aggregated_content += f"\n=== ä¾†æº {idx+1}: {title} ({url}) ===\n{content}\n"
            used_sources.append({"title": title, "url": url})

    if not aggregated_content:
        print("âš ï¸ ç„¡æ³•å¾ä»»ä½•æœå°‹çµæœä¸­æå–æœ‰æ•ˆæ–‡å­—ã€‚")
        return {"summary": "ç„¡æ³•å¾æœå°‹çµæœä¸­æå–æœ‰æ•ˆæ–‡å­—ã€‚", "sources": []}

    # --- 3. åŸ·è¡Œæ¿ƒç¸® (LLM) ---
    print("ğŸ§  [æ¿ƒç¸®] æ­£åœ¨æ•´ç†è³‡è¨Š...")
    
    # ä¿®æ”¹é‡é»ï¼šPrompt æ”¹ç‚ºè‹±æ–‡ï¼Œä¸¦å¼·åˆ¶è¦æ±‚è¼¸å‡ºç¹é«”ä¸­æ–‡
    system_prompt = (
        "You are a professional researcher. "
        "Read the provided raw web data and extract the 3-5 most relevant key points "
        "based on the user's question. Ignore ads and irrelevant noise. "
        "IMPORTANT: You must output the final summary in Traditional Chinese (ç¹é«”ä¸­æ–‡)."
    )

    user_prompt = f"""
    User Question: {query}

    --- Web Collected Data ---
    {aggregated_content}
    """

    # ä½¿ç”¨ await éåŒæ­¥å‘¼å« OpenAI
    response = await client.chat.completions.create(
        model=model_name,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
    )

    summary = response.choices[0].message.content
    return {"summary": summary, "sources": used_sources}


async def process(client: AsyncOpenAI, model_name: str, messages: List[Dict[str, str]]) -> Dict[str, Any]:
    """æ•´åˆå…¥å£ï¼šç”¨æ–¼ä¸»ç³»çµ± router çš„æœå°‹æ¨¡çµ„ã€‚

    æ­¥é©Ÿï¼š
    1. å¾å°è©±æ­·å²ä¸­æŠ“å‡ºæœ€æ–°ä¸€å‰‡ user å•å¥ã€‚
    2. è‹¥å•é¡Œè·ŸåŸä½æ°‘æ—ï¼æ’ç£æ—ç›¸é—œï¼Œå¼·åŒ–æœå°‹é—œéµå­—ã€‚
    3. ä»¥ï¼ˆå¯èƒ½åŠ æ¬Šå¾Œçš„ï¼‰å•å¥ä½œç‚º query å‘¼å« get_web_summaryã€‚
    4. å›å‚³ç¬¦åˆä¸»ç³»çµ±æ ¼å¼çš„ {"reply", "thinking"}ã€‚
    """

    # 1. æŠ“æœ€æ–°ä¸€å‰‡ user å•å¥ä½œç‚ºæœå°‹é—œéµå­—
    user_question = ""
    for msg in reversed(messages):
        if msg.get("role") == "user":
            user_question = str(msg.get("content", "")).strip()
            if user_question:
                break

    if not user_question:
        return {
            "reply": "æ²’æœ‰æ‰¾åˆ°å¯ä»¥ç”¨ä¾†æœå°‹çš„ä½¿ç”¨è€…å•é¡Œã€‚",
            "thinking": "Search module: no user question detected.",
        }

    # 2. æ ¹æ“šé—œéµå­—åˆ¤æ–·æ˜¯å¦ç‚ºåŸä½æ°‘æ—ï¼æ’ç£æ—ç›¸é—œæŸ¥è©¢ï¼Œè‹¥æ˜¯å‰‡åŠ å¼·é—œéµå­—
    indigenous_keywords = [
        "æ’ç£", "æ’ç£æ—", "paiwan", "åŸä½æ°‘", "åŸæ°‘", "æ—èª", "æ¯èª", "å—å³¶èª",
        "é˜¿ç¾æ—", "æ³°é›…æ—", "å¸ƒè¾²æ—", "é­¯å‡±æ—", "å‘å—æ—", "é„’æ—", "è³½å¤æ—",
        "äº”å¹´ç¥­", "äº”å¹´ç¥­å…¸", "äº”å¹´å¤§ç¥­",
    ]

    is_indigenous_question = any(k.lower() in user_question.lower() for k in indigenous_keywords)

    # 2.5 åªå¾ä½¿ç”¨è€…å•é¡Œæœ¬èº«æŠ“é—œéµè©ï¼Œä¸å†è®“ LLM ç”¢ç”Ÿ query
    # å„ªå…ˆæŠ“å‡ºåœ¨ indigenous_keywords è£¡å‡ºç¾çš„è©ï¼Œä¾‹å¦‚ã€Œäº”å¹´ç¥­ã€ã€ã€Œæ’ç£æ—ã€
    base_query = user_question
    lower_q = user_question.lower()
    matched_keywords: List[str] = []
    for kw in indigenous_keywords:
        if kw.lower() in lower_q and kw not in matched_keywords:
            matched_keywords.append(kw)

    if matched_keywords:
        # ä¾‹å¦‚ã€Œä½ èƒ½ä»‹ç´¹ä¸€ä¸‹äº”å¹´ç¥­å—ï¼Ÿã€ -> "äº”å¹´ç¥­"
        base_query = " ".join(matched_keywords)

    # 3. å‘¼å« web æœå°‹èˆ‡æ‘˜è¦ï¼ˆä¸å†é¡å¤–é™„åŠ é•·ä¸²é—œéµå­—ï¼‰
    web_result = await get_web_summary(client, model_name, messages, base_query)
    summary = web_result.get("summary", "")
    sources = web_result.get("sources", [])

    # 4. ä¾ç…§ç¾æœ‰ UI æ ¼å¼å›å‚³ï¼Œä¸¦æŠŠå¯¦éš›ä½¿ç”¨åˆ°çš„ä¾†æºç¶²ç«™åˆ—åœ¨ thinking è£¡
    thinking_lines = [
        f"å·²é‡å°ã€Œ{user_question}ã€é€é DuckDuckGo é€²è¡Œç¶²è·¯æœå°‹ä¸¦æ•´ç†é‡é»ã€‚"
        + ("ï¼ˆå·²é‡å°åŸä½æ°‘æ—ï¼æ’ç£æ—ç›¸é—œä¸»é¡ŒåŠ å¼·é—œéµå­—ã€‚)" if is_indigenous_question else ""),
    ]

    if sources:
        thinking_lines.append("ä½¿ç”¨çš„ä¸»è¦è³‡æ–™ä¾†æºï¼š")
        for src in sources:
            title = src.get("title") or "(ç„¡æ¨™é¡Œ)"
            url = src.get("url") or "(ç„¡ç¶²å€)"
            thinking_lines.append(f"- {title} ({url})")

    thinking = "\n".join(thinking_lines)

    return {
        "reply": summary,
        "thinking": thinking,
    }